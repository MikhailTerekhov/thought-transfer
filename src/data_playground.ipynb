{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3d1c2f62cab179f1",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-31T01:08:26.746600Z",
     "start_time": "2024-08-31T01:08:06.122165Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\r\n",
      "Looking in indexes: https://pypi.org/simple, https://pypi.ngc.nvidia.com\r\n",
      "Collecting git+https://github.com/TransformerLensOrg/TransformerLens\r\n",
      "  Cloning https://github.com/TransformerLensOrg/TransformerLens to /tmp/pip-req-build-oyt_na_i\r\n",
      "  Running command git clone --filter=blob:none --quiet https://github.com/TransformerLensOrg/TransformerLens /tmp/pip-req-build-oyt_na_i\r\n",
      "  Resolved https://github.com/TransformerLensOrg/TransformerLens to commit cb5017ad0f30cde0d3ac0b0f863c27fbec964c28\r\n",
      "  Installing build dependencies ... \u001b[?25ldone\r\n",
      "\u001b[?25h  Getting requirements to build wheel ... \u001b[?25ldone\r\n",
      "\u001b[?25h  Preparing metadata (pyproject.toml) ... \u001b[?25ldone\r\n",
      "\u001b[?25hRequirement already satisfied: accelerate>=0.23.0 in /home/terekhov/.local/lib/python3.10/site-packages (from transformer-lens==0.0.0) (0.33.0)\r\n",
      "Requirement already satisfied: beartype<0.15.0,>=0.14.1 in /home/terekhov/.local/lib/python3.10/site-packages (from transformer-lens==0.0.0) (0.14.1)\r\n",
      "Requirement already satisfied: better-abc<0.0.4,>=0.0.3 in /home/terekhov/.local/lib/python3.10/site-packages (from transformer-lens==0.0.0) (0.0.3)\r\n",
      "Requirement already satisfied: datasets>=2.7.1 in /home/terekhov/.local/lib/python3.10/site-packages (from transformer-lens==0.0.0) (2.21.0)\r\n",
      "Requirement already satisfied: einops>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from transformer-lens==0.0.0) (0.8.0)\r\n",
      "Requirement already satisfied: fancy-einsum>=0.0.3 in /home/terekhov/.local/lib/python3.10/site-packages (from transformer-lens==0.0.0) (0.0.3)\r\n",
      "Requirement already satisfied: jaxtyping>=0.2.11 in /home/terekhov/.local/lib/python3.10/site-packages (from transformer-lens==0.0.0) (0.2.33)\r\n",
      "Requirement already satisfied: numpy>=1.24 in /usr/local/lib/python3.10/dist-packages (from transformer-lens==0.0.0) (1.24.4)\r\n",
      "Requirement already satisfied: pandas>=1.1.5 in /usr/local/lib/python3.10/dist-packages (from transformer-lens==0.0.0) (2.2.1)\r\n",
      "Requirement already satisfied: rich>=12.6.0 in /usr/local/lib/python3.10/dist-packages (from transformer-lens==0.0.0) (13.7.1)\r\n",
      "Requirement already satisfied: sentencepiece in /home/terekhov/.local/lib/python3.10/site-packages (from transformer-lens==0.0.0) (0.2.0)\r\n",
      "Requirement already satisfied: torch>=1.10 in /usr/local/lib/python3.10/dist-packages (from transformer-lens==0.0.0) (2.4.0a0+f70bd71a48.nv24.6)\r\n",
      "Requirement already satisfied: tqdm>=4.64.1 in /usr/local/lib/python3.10/dist-packages (from transformer-lens==0.0.0) (4.66.4)\r\n",
      "Requirement already satisfied: transformers>=4.37.2 in /home/terekhov/.local/lib/python3.10/site-packages (from transformer-lens==0.0.0) (4.44.2)\r\n",
      "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from transformer-lens==0.0.0) (4.12.0)\r\n",
      "Requirement already satisfied: wandb>=0.13.5 in /usr/local/lib/python3.10/dist-packages (from transformer-lens==0.0.0) (0.17.8)\r\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from accelerate>=0.23.0->transformer-lens==0.0.0) (24.0)\r\n",
      "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from accelerate>=0.23.0->transformer-lens==0.0.0) (5.9.8)\r\n",
      "Requirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from accelerate>=0.23.0->transformer-lens==0.0.0) (6.0.1)\r\n",
      "Requirement already satisfied: huggingface-hub>=0.21.0 in /home/terekhov/.local/lib/python3.10/site-packages (from accelerate>=0.23.0->transformer-lens==0.0.0) (0.24.6)\r\n",
      "Requirement already satisfied: safetensors>=0.3.1 in /home/terekhov/.local/lib/python3.10/site-packages (from accelerate>=0.23.0->transformer-lens==0.0.0) (0.4.4)\r\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from datasets>=2.7.1->transformer-lens==0.0.0) (3.14.0)\r\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in /home/terekhov/.local/lib/python3.10/site-packages (from datasets>=2.7.1->transformer-lens==0.0.0) (17.0.0)\r\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /home/terekhov/.local/lib/python3.10/site-packages (from datasets>=2.7.1->transformer-lens==0.0.0) (0.3.8)\r\n",
      "Requirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.10/dist-packages (from datasets>=2.7.1->transformer-lens==0.0.0) (2.32.3)\r\n",
      "Requirement already satisfied: xxhash in /home/terekhov/.local/lib/python3.10/site-packages (from datasets>=2.7.1->transformer-lens==0.0.0) (3.5.0)\r\n",
      "Requirement already satisfied: multiprocess in /home/terekhov/.local/lib/python3.10/site-packages (from datasets>=2.7.1->transformer-lens==0.0.0) (0.70.16)\r\n",
      "Requirement already satisfied: fsspec<=2024.6.1,>=2023.1.0 in /usr/local/lib/python3.10/dist-packages (from fsspec[http]<=2024.6.1,>=2023.1.0->datasets>=2.7.1->transformer-lens==0.0.0) (2024.5.0)\r\n",
      "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets>=2.7.1->transformer-lens==0.0.0) (3.9.5)\r\n",
      "Requirement already satisfied: typeguard==2.13.3 in /home/terekhov/.local/lib/python3.10/site-packages (from jaxtyping>=0.2.11->transformer-lens==0.0.0) (2.13.3)\r\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.1.5->transformer-lens==0.0.0) (2.9.0.post0)\r\n",
      "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.1.5->transformer-lens==0.0.0) (2024.1)\r\n",
      "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.1.5->transformer-lens==0.0.0) (2024.1)\r\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich>=12.6.0->transformer-lens==0.0.0) (3.0.0)\r\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich>=12.6.0->transformer-lens==0.0.0) (2.18.0)\r\n",
      "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.10->transformer-lens==0.0.0) (1.12.1)\r\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.10->transformer-lens==0.0.0) (3.3)\r\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10->transformer-lens==0.0.0) (3.1.4)\r\n",
      "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers>=4.37.2->transformer-lens==0.0.0) (2024.5.15)\r\n",
      "Requirement already satisfied: tokenizers<0.20,>=0.19 in /home/terekhov/.local/lib/python3.10/site-packages (from transformers>=4.37.2->transformer-lens==0.0.0) (0.19.1)\r\n",
      "Requirement already satisfied: click!=8.0.0,>=7.1 in /usr/local/lib/python3.10/dist-packages (from wandb>=0.13.5->transformer-lens==0.0.0) (8.1.7)\r\n",
      "Requirement already satisfied: docker-pycreds>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from wandb>=0.13.5->transformer-lens==0.0.0) (0.4.0)\r\n",
      "Requirement already satisfied: gitpython!=3.1.29,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from wandb>=0.13.5->transformer-lens==0.0.0) (3.1.43)\r\n",
      "Requirement already satisfied: platformdirs in /usr/local/lib/python3.10/dist-packages (from wandb>=0.13.5->transformer-lens==0.0.0) (4.2.2)\r\n",
      "Requirement already satisfied: protobuf!=4.21.0,<6,>=3.19.0 in /usr/local/lib/python3.10/dist-packages (from wandb>=0.13.5->transformer-lens==0.0.0) (4.24.4)\r\n",
      "Requirement already satisfied: sentry-sdk>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from wandb>=0.13.5->transformer-lens==0.0.0) (2.13.0)\r\n",
      "Requirement already satisfied: setproctitle in /usr/local/lib/python3.10/dist-packages (from wandb>=0.13.5->transformer-lens==0.0.0) (1.3.3)\r\n",
      "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from wandb>=0.13.5->transformer-lens==0.0.0) (68.2.2)\r\n",
      "Requirement already satisfied: six>=1.4.0 in /usr/local/lib/python3.10/dist-packages (from docker-pycreds>=0.4.0->wandb>=0.13.5->transformer-lens==0.0.0) (1.16.0)\r\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.7.1->transformer-lens==0.0.0) (1.3.1)\r\n",
      "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.7.1->transformer-lens==0.0.0) (23.2.0)\r\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.7.1->transformer-lens==0.0.0) (1.4.1)\r\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.7.1->transformer-lens==0.0.0) (6.0.5)\r\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.7.1->transformer-lens==0.0.0) (1.9.4)\r\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.7.1->transformer-lens==0.0.0) (4.0.3)\r\n",
      "Requirement already satisfied: gitdb<5,>=4.0.1 in /usr/local/lib/python3.10/dist-packages (from gitpython!=3.1.29,>=1.0.0->wandb>=0.13.5->transformer-lens==0.0.0) (4.0.11)\r\n",
      "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich>=12.6.0->transformer-lens==0.0.0) (0.1.2)\r\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets>=2.7.1->transformer-lens==0.0.0) (3.3.2)\r\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets>=2.7.1->transformer-lens==0.0.0) (3.7)\r\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets>=2.7.1->transformer-lens==0.0.0) (2.0.7)\r\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets>=2.7.1->transformer-lens==0.0.0) (2024.6.2)\r\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.10->transformer-lens==0.0.0) (2.1.5)\r\n",
      "Requirement already satisfied: mpmath<1.4.0,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.10->transformer-lens==0.0.0) (1.3.0)\r\n",
      "Requirement already satisfied: smmap<6,>=3.0.1 in /usr/local/lib/python3.10/dist-packages (from gitdb<5,>=4.0.1->gitpython!=3.1.29,>=1.0.0->wandb>=0.13.5->transformer-lens==0.0.0) (5.0.1)\r\n",
      "\r\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.0\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.2\u001b[0m\r\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython -m pip install --upgrade pip\u001b[0m\r\n"
     ]
    }
   ],
   "source": [
    "!pip install git+https://github.com/TransformerLensOrg/TransformerLens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f52813ee580c947b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-31T01:08:37.218034Z",
     "start_time": "2024-08-31T01:08:26.858251Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\r\n",
      "Looking in indexes: https://pypi.org/simple, https://pypi.ngc.nvidia.com\r\n",
      "Collecting ipywidgets\r\n",
      "  Downloading ipywidgets-8.1.5-py3-none-any.whl.metadata (2.3 kB)\r\n",
      "Requirement already satisfied: comm>=0.1.3 in /usr/local/lib/python3.10/dist-packages (from ipywidgets) (0.2.2)\r\n",
      "Requirement already satisfied: ipython>=6.1.0 in /usr/local/lib/python3.10/dist-packages (from ipywidgets) (8.21.0)\r\n",
      "Requirement already satisfied: traitlets>=4.3.1 in /usr/local/lib/python3.10/dist-packages (from ipywidgets) (5.9.0)\r\n",
      "Collecting widgetsnbextension~=4.0.12 (from ipywidgets)\r\n",
      "  Downloading widgetsnbextension-4.0.13-py3-none-any.whl.metadata (1.6 kB)\r\n",
      "Collecting jupyterlab-widgets~=3.0.12 (from ipywidgets)\r\n",
      "  Downloading jupyterlab_widgets-3.0.13-py3-none-any.whl.metadata (4.1 kB)\r\n",
      "Requirement already satisfied: decorator in /usr/local/lib/python3.10/dist-packages (from ipython>=6.1.0->ipywidgets) (5.1.1)\r\n",
      "Requirement already satisfied: jedi>=0.16 in /usr/local/lib/python3.10/dist-packages (from ipython>=6.1.0->ipywidgets) (0.19.1)\r\n",
      "Requirement already satisfied: matplotlib-inline in /usr/local/lib/python3.10/dist-packages (from ipython>=6.1.0->ipywidgets) (0.1.7)\r\n",
      "Requirement already satisfied: prompt-toolkit<3.1.0,>=3.0.41 in /usr/local/lib/python3.10/dist-packages (from ipython>=6.1.0->ipywidgets) (3.0.45)\r\n",
      "Requirement already satisfied: pygments>=2.4.0 in /usr/local/lib/python3.10/dist-packages (from ipython>=6.1.0->ipywidgets) (2.18.0)\r\n",
      "Requirement already satisfied: stack-data in /usr/local/lib/python3.10/dist-packages (from ipython>=6.1.0->ipywidgets) (0.6.3)\r\n",
      "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from ipython>=6.1.0->ipywidgets) (1.2.1)\r\n",
      "Requirement already satisfied: pexpect>4.3 in /usr/local/lib/python3.10/dist-packages (from ipython>=6.1.0->ipywidgets) (4.9.0)\r\n",
      "Requirement already satisfied: parso<0.9.0,>=0.8.3 in /usr/local/lib/python3.10/dist-packages (from jedi>=0.16->ipython>=6.1.0->ipywidgets) (0.8.4)\r\n",
      "Requirement already satisfied: ptyprocess>=0.5 in /usr/local/lib/python3.10/dist-packages (from pexpect>4.3->ipython>=6.1.0->ipywidgets) (0.7.0)\r\n",
      "Requirement already satisfied: wcwidth in /usr/local/lib/python3.10/dist-packages (from prompt-toolkit<3.1.0,>=3.0.41->ipython>=6.1.0->ipywidgets) (0.2.13)\r\n",
      "Requirement already satisfied: executing>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from stack-data->ipython>=6.1.0->ipywidgets) (2.0.1)\r\n",
      "Requirement already satisfied: asttokens>=2.1.0 in /usr/local/lib/python3.10/dist-packages (from stack-data->ipython>=6.1.0->ipywidgets) (2.4.1)\r\n",
      "Requirement already satisfied: pure-eval in /usr/local/lib/python3.10/dist-packages (from stack-data->ipython>=6.1.0->ipywidgets) (0.2.2)\r\n",
      "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from asttokens>=2.1.0->stack-data->ipython>=6.1.0->ipywidgets) (1.16.0)\r\n",
      "Downloading ipywidgets-8.1.5-py3-none-any.whl (139 kB)\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m139.8/139.8 kB\u001b[0m \u001b[31m7.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hDownloading jupyterlab_widgets-3.0.13-py3-none-any.whl (214 kB)\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m214.4/214.4 kB\u001b[0m \u001b[31m10.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hDownloading widgetsnbextension-4.0.13-py3-none-any.whl (2.3 MB)\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.3/2.3 MB\u001b[0m \u001b[31m14.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\r\n",
      "\u001b[?25hInstalling collected packages: widgetsnbextension, jupyterlab-widgets, ipywidgets\r\n",
      "Successfully installed ipywidgets-8.1.5 jupyterlab-widgets-3.0.13 widgetsnbextension-4.0.13\r\n",
      "\r\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.0\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.2\u001b[0m\r\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython -m pip install --upgrade pip\u001b[0m\r\n"
     ]
    }
   ],
   "source": [
    "!pip install ipywidgets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b40592accab7f4ce",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-31T01:08:58.394581Z",
     "start_time": "2024-08-31T01:08:37.684973Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\r\n",
      "Looking in indexes: https://pypi.org/simple, https://pypi.ngc.nvidia.com\r\n",
      "Requirement already satisfied: webdataset in /home/terekhov/.local/lib/python3.10/site-packages (0.2.96)\r\n",
      "Requirement already satisfied: braceexpand in /home/terekhov/.local/lib/python3.10/site-packages (from webdataset) (0.1.7)\r\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from webdataset) (1.24.4)\r\n",
      "Requirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from webdataset) (6.0.1)\r\n",
      "\r\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.0\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.2\u001b[0m\r\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython -m pip install --upgrade pip\u001b[0m\r\n",
      "Defaulting to user installation because normal site-packages is not writeable\r\n",
      "Looking in indexes: https://pypi.org/simple, https://pypi.ngc.nvidia.com\r\n",
      "Requirement already satisfied: datasets in /home/terekhov/.local/lib/python3.10/site-packages (2.21.0)\r\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from datasets) (3.14.0)\r\n",
      "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from datasets) (1.24.4)\r\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in /home/terekhov/.local/lib/python3.10/site-packages (from datasets) (17.0.0)\r\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /home/terekhov/.local/lib/python3.10/site-packages (from datasets) (0.3.8)\r\n",
      "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets) (2.2.1)\r\n",
      "Requirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.10/dist-packages (from datasets) (2.32.3)\r\n",
      "Requirement already satisfied: tqdm>=4.66.3 in /usr/local/lib/python3.10/dist-packages (from datasets) (4.66.4)\r\n",
      "Requirement already satisfied: xxhash in /home/terekhov/.local/lib/python3.10/site-packages (from datasets) (3.5.0)\r\n",
      "Requirement already satisfied: multiprocess in /home/terekhov/.local/lib/python3.10/site-packages (from datasets) (0.70.16)\r\n",
      "Requirement already satisfied: fsspec<=2024.6.1,>=2023.1.0 in /usr/local/lib/python3.10/dist-packages (from fsspec[http]<=2024.6.1,>=2023.1.0->datasets) (2024.5.0)\r\n",
      "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets) (3.9.5)\r\n",
      "Requirement already satisfied: huggingface-hub>=0.21.2 in /home/terekhov/.local/lib/python3.10/site-packages (from datasets) (0.24.6)\r\n",
      "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from datasets) (24.0)\r\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from datasets) (6.0.1)\r\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.3.1)\r\n",
      "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (23.2.0)\r\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.4.1)\r\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (6.0.5)\r\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.9.4)\r\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (4.0.3)\r\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.21.2->datasets) (4.12.0)\r\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (3.3.2)\r\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (3.7)\r\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (2.0.7)\r\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (2024.6.2)\r\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2.9.0.post0)\r\n",
      "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2024.1)\r\n",
      "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2024.1)\r\n",
      "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.16.0)\r\n",
      "\r\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.0\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.2\u001b[0m\r\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython -m pip install --upgrade pip\u001b[0m\r\n",
      "Defaulting to user installation because normal site-packages is not writeable\r\n",
      "Looking in indexes: https://pypi.org/simple, https://pypi.ngc.nvidia.com\r\n",
      "Requirement already satisfied: nltk in /home/terekhov/.local/lib/python3.10/site-packages (3.9.1)\r\n",
      "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk) (8.1.7)\r\n",
      "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk) (1.4.2)\r\n",
      "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk) (2024.5.15)\r\n",
      "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from nltk) (4.66.4)\r\n",
      "\r\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.0\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.2\u001b[0m\r\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython -m pip install --upgrade pip\u001b[0m\r\n"
     ]
    }
   ],
   "source": [
    "!pip install webdataset\n",
    "!pip install datasets\n",
    "!pip install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "initial_id",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-31T01:09:20.225396Z",
     "start_time": "2024-08-31T01:08:58.515605Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "from pathlib import Path\n",
    "from io import BytesIO\n",
    "import json\n",
    "\n",
    "import torch as t\n",
    "import numpy as np\n",
    "import transformer_lens as tl\n",
    "import webdataset as wds\n",
    "import datasets\n",
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9563aa95908756ff",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-31T01:09:24.967834Z",
     "start_time": "2024-08-31T01:09:20.564102Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/terekhov/.local/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained model opt-125m into HookedTransformer\n",
      "Loaded pretrained model gpt2-small into HookedTransformer\n"
     ]
    }
   ],
   "source": [
    "opt_small = tl.HookedTransformer.from_pretrained('opt-125m')\n",
    "gpt2_small = tl.HookedTransformer.from_pretrained('gpt2-small')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5744ed982630c061",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-31T01:09:25.235703Z",
     "start_time": "2024-08-31T01:09:25.051714Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['hook_embed', 'hook_pos_embed', 'blocks.0.hook_resid_pre', 'blocks.0.ln1.hook_scale', 'blocks.0.ln1.hook_normalized', 'blocks.0.attn.hook_q', 'blocks.0.attn.hook_k', 'blocks.0.attn.hook_v', 'blocks.0.attn.hook_attn_scores', 'blocks.0.attn.hook_pattern', 'blocks.0.attn.hook_z', 'blocks.0.hook_attn_out', 'blocks.0.hook_resid_mid', 'blocks.0.ln2.hook_scale', 'blocks.0.ln2.hook_normalized', 'blocks.0.mlp.hook_pre', 'blocks.0.mlp.hook_post', 'blocks.0.hook_mlp_out', 'blocks.0.hook_resid_post', 'blocks.1.hook_resid_pre', 'blocks.1.ln1.hook_scale', 'blocks.1.ln1.hook_normalized', 'blocks.1.attn.hook_q', 'blocks.1.attn.hook_k', 'blocks.1.attn.hook_v', 'blocks.1.attn.hook_attn_scores', 'blocks.1.attn.hook_pattern', 'blocks.1.attn.hook_z', 'blocks.1.hook_attn_out', 'blocks.1.hook_resid_mid', 'blocks.1.ln2.hook_scale', 'blocks.1.ln2.hook_normalized', 'blocks.1.mlp.hook_pre', 'blocks.1.mlp.hook_post', 'blocks.1.hook_mlp_out', 'blocks.1.hook_resid_post', 'blocks.2.hook_resid_pre', 'blocks.2.ln1.hook_scale', 'blocks.2.ln1.hook_normalized', 'blocks.2.attn.hook_q', 'blocks.2.attn.hook_k', 'blocks.2.attn.hook_v', 'blocks.2.attn.hook_attn_scores', 'blocks.2.attn.hook_pattern', 'blocks.2.attn.hook_z', 'blocks.2.hook_attn_out', 'blocks.2.hook_resid_mid', 'blocks.2.ln2.hook_scale', 'blocks.2.ln2.hook_normalized', 'blocks.2.mlp.hook_pre', 'blocks.2.mlp.hook_post', 'blocks.2.hook_mlp_out', 'blocks.2.hook_resid_post', 'blocks.3.hook_resid_pre', 'blocks.3.ln1.hook_scale', 'blocks.3.ln1.hook_normalized', 'blocks.3.attn.hook_q', 'blocks.3.attn.hook_k', 'blocks.3.attn.hook_v', 'blocks.3.attn.hook_attn_scores', 'blocks.3.attn.hook_pattern', 'blocks.3.attn.hook_z', 'blocks.3.hook_attn_out', 'blocks.3.hook_resid_mid', 'blocks.3.ln2.hook_scale', 'blocks.3.ln2.hook_normalized', 'blocks.3.mlp.hook_pre', 'blocks.3.mlp.hook_post', 'blocks.3.hook_mlp_out', 'blocks.3.hook_resid_post', 'blocks.4.hook_resid_pre', 'blocks.4.ln1.hook_scale', 'blocks.4.ln1.hook_normalized', 'blocks.4.attn.hook_q', 'blocks.4.attn.hook_k', 'blocks.4.attn.hook_v', 'blocks.4.attn.hook_attn_scores', 'blocks.4.attn.hook_pattern', 'blocks.4.attn.hook_z', 'blocks.4.hook_attn_out', 'blocks.4.hook_resid_mid', 'blocks.4.ln2.hook_scale', 'blocks.4.ln2.hook_normalized', 'blocks.4.mlp.hook_pre', 'blocks.4.mlp.hook_post', 'blocks.4.hook_mlp_out', 'blocks.4.hook_resid_post', 'blocks.5.hook_resid_pre', 'blocks.5.ln1.hook_scale', 'blocks.5.ln1.hook_normalized', 'blocks.5.attn.hook_q', 'blocks.5.attn.hook_k', 'blocks.5.attn.hook_v', 'blocks.5.attn.hook_attn_scores', 'blocks.5.attn.hook_pattern', 'blocks.5.attn.hook_z', 'blocks.5.hook_attn_out', 'blocks.5.hook_resid_mid', 'blocks.5.ln2.hook_scale', 'blocks.5.ln2.hook_normalized', 'blocks.5.mlp.hook_pre', 'blocks.5.mlp.hook_post', 'blocks.5.hook_mlp_out', 'blocks.5.hook_resid_post', 'blocks.6.hook_resid_pre', 'blocks.6.ln1.hook_scale', 'blocks.6.ln1.hook_normalized', 'blocks.6.attn.hook_q', 'blocks.6.attn.hook_k', 'blocks.6.attn.hook_v', 'blocks.6.attn.hook_attn_scores', 'blocks.6.attn.hook_pattern', 'blocks.6.attn.hook_z', 'blocks.6.hook_attn_out', 'blocks.6.hook_resid_mid', 'blocks.6.ln2.hook_scale', 'blocks.6.ln2.hook_normalized', 'blocks.6.mlp.hook_pre', 'blocks.6.mlp.hook_post', 'blocks.6.hook_mlp_out', 'blocks.6.hook_resid_post', 'blocks.7.hook_resid_pre', 'blocks.7.ln1.hook_scale', 'blocks.7.ln1.hook_normalized', 'blocks.7.attn.hook_q', 'blocks.7.attn.hook_k', 'blocks.7.attn.hook_v', 'blocks.7.attn.hook_attn_scores', 'blocks.7.attn.hook_pattern', 'blocks.7.attn.hook_z', 'blocks.7.hook_attn_out', 'blocks.7.hook_resid_mid', 'blocks.7.ln2.hook_scale', 'blocks.7.ln2.hook_normalized', 'blocks.7.mlp.hook_pre', 'blocks.7.mlp.hook_post', 'blocks.7.hook_mlp_out', 'blocks.7.hook_resid_post', 'blocks.8.hook_resid_pre', 'blocks.8.ln1.hook_scale', 'blocks.8.ln1.hook_normalized', 'blocks.8.attn.hook_q', 'blocks.8.attn.hook_k', 'blocks.8.attn.hook_v', 'blocks.8.attn.hook_attn_scores', 'blocks.8.attn.hook_pattern', 'blocks.8.attn.hook_z', 'blocks.8.hook_attn_out', 'blocks.8.hook_resid_mid', 'blocks.8.ln2.hook_scale', 'blocks.8.ln2.hook_normalized', 'blocks.8.mlp.hook_pre', 'blocks.8.mlp.hook_post', 'blocks.8.hook_mlp_out', 'blocks.8.hook_resid_post', 'blocks.9.hook_resid_pre', 'blocks.9.ln1.hook_scale', 'blocks.9.ln1.hook_normalized', 'blocks.9.attn.hook_q', 'blocks.9.attn.hook_k', 'blocks.9.attn.hook_v', 'blocks.9.attn.hook_attn_scores', 'blocks.9.attn.hook_pattern', 'blocks.9.attn.hook_z', 'blocks.9.hook_attn_out', 'blocks.9.hook_resid_mid', 'blocks.9.ln2.hook_scale', 'blocks.9.ln2.hook_normalized', 'blocks.9.mlp.hook_pre', 'blocks.9.mlp.hook_post', 'blocks.9.hook_mlp_out', 'blocks.9.hook_resid_post', 'blocks.10.hook_resid_pre', 'blocks.10.ln1.hook_scale', 'blocks.10.ln1.hook_normalized', 'blocks.10.attn.hook_q', 'blocks.10.attn.hook_k', 'blocks.10.attn.hook_v', 'blocks.10.attn.hook_attn_scores', 'blocks.10.attn.hook_pattern', 'blocks.10.attn.hook_z', 'blocks.10.hook_attn_out', 'blocks.10.hook_resid_mid', 'blocks.10.ln2.hook_scale', 'blocks.10.ln2.hook_normalized', 'blocks.10.mlp.hook_pre', 'blocks.10.mlp.hook_post', 'blocks.10.hook_mlp_out', 'blocks.10.hook_resid_post', 'blocks.11.hook_resid_pre', 'blocks.11.ln1.hook_scale', 'blocks.11.ln1.hook_normalized', 'blocks.11.attn.hook_q', 'blocks.11.attn.hook_k', 'blocks.11.attn.hook_v', 'blocks.11.attn.hook_attn_scores', 'blocks.11.attn.hook_pattern', 'blocks.11.attn.hook_z', 'blocks.11.hook_attn_out', 'blocks.11.hook_resid_mid', 'blocks.11.ln2.hook_scale', 'blocks.11.ln2.hook_normalized', 'blocks.11.mlp.hook_pre', 'blocks.11.mlp.hook_post', 'blocks.11.hook_mlp_out', 'blocks.11.hook_resid_post', 'ln_final.hook_scale', 'ln_final.hook_normalized'])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = \"This project will look into cross-model similarity in the hidden representations in the residual stream. The main outcomes are\"\n",
    "logits, cache = opt_small.run_with_cache(text)\n",
    "cache.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "62a35643e1c5467c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-31T01:09:25.301162Z",
     "start_time": "2024-08-31T01:09:25.294198Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['</s>', 'This', ' project', ' will', ' look', ' into', ' cross', '-', 'model', ' similarity', ' in', ' the', ' hidden', ' representations', ' in', ' the', ' residual', ' stream', '.', ' The', ' main', ' outcomes', ' are']\n",
      "['<|endoftext|>', 'This', ' project', ' will', ' look', ' into', ' cross', '-', 'model', ' similarity', ' in', ' the', ' hidden', ' representations', ' in', ' the', ' residual', ' stream', '.', ' The', ' main', ' outcomes', ' are']\n"
     ]
    }
   ],
   "source": [
    "print(opt_small.to_str_tokens(text))\n",
    "print(gpt2_small.to_str_tokens(text))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2eb8c933529692f6",
   "metadata": {},
   "source": [
    "Aha, I'm guessing OPT has a very similar tokenization scheme. I can filter examples where the tokenization of the sentence is the same."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3ab94ea9e1a1c16b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-31T01:09:25.375014Z",
     "start_time": "2024-08-31T01:09:25.365820Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([12, 12, 768, 64])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "opt_small.W_K.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "35af726ffe75c158",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-31T01:09:25.511915Z",
     "start_time": "2024-08-31T01:09:25.439306Z"
    }
   },
   "outputs": [],
   "source": [
    "nlayers = opt_small.W_K.shape[0]\n",
    "mid_layer = nlayers // 2\n",
    "d_model = opt_small.W_K.shape[2]\n",
    "cached_value = None\n",
    "\n",
    "def cache_it(resid, hook):\n",
    "    global cached_value\n",
    "    cached_value = resid.detach()\n",
    "    # print(hook)\n",
    "    return resid\n",
    "\n",
    "loss = opt_small.run_with_hooks(text, return_type=\"loss\", fwd_hooks=[(f'blocks.{mid_layer}.hook_resid_post', cache_it)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4f79a90b652b2c2b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-31T01:09:25.580070Z",
     "start_time": "2024-08-31T01:09:25.570679Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "HookedTransformer(\n",
       "  (embed): Embed()\n",
       "  (hook_embed): HookPoint()\n",
       "  (pos_embed): PosEmbed()\n",
       "  (hook_pos_embed): HookPoint()\n",
       "  (blocks): ModuleList(\n",
       "    (0-11): 12 x TransformerBlock(\n",
       "      (ln1): LayerNormPre(\n",
       "        (hook_scale): HookPoint()\n",
       "        (hook_normalized): HookPoint()\n",
       "      )\n",
       "      (ln2): LayerNormPre(\n",
       "        (hook_scale): HookPoint()\n",
       "        (hook_normalized): HookPoint()\n",
       "      )\n",
       "      (attn): Attention(\n",
       "        (hook_k): HookPoint()\n",
       "        (hook_q): HookPoint()\n",
       "        (hook_v): HookPoint()\n",
       "        (hook_z): HookPoint()\n",
       "        (hook_attn_scores): HookPoint()\n",
       "        (hook_pattern): HookPoint()\n",
       "        (hook_result): HookPoint()\n",
       "      )\n",
       "      (mlp): MLP(\n",
       "        (hook_pre): HookPoint()\n",
       "        (hook_post): HookPoint()\n",
       "      )\n",
       "      (hook_attn_in): HookPoint()\n",
       "      (hook_q_input): HookPoint()\n",
       "      (hook_k_input): HookPoint()\n",
       "      (hook_v_input): HookPoint()\n",
       "      (hook_mlp_in): HookPoint()\n",
       "      (hook_attn_out): HookPoint()\n",
       "      (hook_mlp_out): HookPoint()\n",
       "      (hook_resid_pre): HookPoint()\n",
       "      (hook_resid_mid): HookPoint()\n",
       "      (hook_resid_post): HookPoint()\n",
       "    )\n",
       "  )\n",
       "  (ln_final): LayerNormPre(\n",
       "    (hook_scale): HookPoint()\n",
       "    (hook_normalized): HookPoint()\n",
       "  )\n",
       "  (unembed): Unembed()\n",
       ")"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "opt_small"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ffdc4b761a970cba",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-31T01:09:25.652119Z",
     "start_time": "2024-08-31T01:09:25.644185Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "HookedTransformer(\n",
       "  (embed): Embed()\n",
       "  (hook_embed): HookPoint()\n",
       "  (pos_embed): PosEmbed()\n",
       "  (hook_pos_embed): HookPoint()\n",
       "  (blocks): ModuleList(\n",
       "    (0-11): 12 x TransformerBlock(\n",
       "      (ln1): LayerNormPre(\n",
       "        (hook_scale): HookPoint()\n",
       "        (hook_normalized): HookPoint()\n",
       "      )\n",
       "      (ln2): LayerNormPre(\n",
       "        (hook_scale): HookPoint()\n",
       "        (hook_normalized): HookPoint()\n",
       "      )\n",
       "      (attn): Attention(\n",
       "        (hook_k): HookPoint()\n",
       "        (hook_q): HookPoint()\n",
       "        (hook_v): HookPoint()\n",
       "        (hook_z): HookPoint()\n",
       "        (hook_attn_scores): HookPoint()\n",
       "        (hook_pattern): HookPoint()\n",
       "        (hook_result): HookPoint()\n",
       "      )\n",
       "      (mlp): MLP(\n",
       "        (hook_pre): HookPoint()\n",
       "        (hook_post): HookPoint()\n",
       "      )\n",
       "      (hook_attn_in): HookPoint()\n",
       "      (hook_q_input): HookPoint()\n",
       "      (hook_k_input): HookPoint()\n",
       "      (hook_v_input): HookPoint()\n",
       "      (hook_mlp_in): HookPoint()\n",
       "      (hook_attn_out): HookPoint()\n",
       "      (hook_mlp_out): HookPoint()\n",
       "      (hook_resid_pre): HookPoint()\n",
       "      (hook_resid_mid): HookPoint()\n",
       "      (hook_resid_post): HookPoint()\n",
       "    )\n",
       "  )\n",
       "  (ln_final): LayerNormPre(\n",
       "    (hook_scale): HookPoint()\n",
       "    (hook_normalized): HookPoint()\n",
       "  )\n",
       "  (unembed): Unembed()\n",
       ")"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gpt2_small"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f3dbe47c31563b8c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-31T01:09:25.724873Z",
     "start_time": "2024-08-31T01:09:25.718804Z"
    }
   },
   "outputs": [],
   "source": [
    "cached_value.reshape((-1, d_model)).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8f2677276106833c",
   "metadata": {},
   "outputs": [],
   "source": [
    "texts = [\"This project will look into cross-model similarity in the hidden representations in the residual stream. The main outcomes are\",\n",
    "         \"A powerful AI model that can generate text from a prompt. The model is trained on a large corpus of text data.\",\n",
    "         \"This text was generated by Copilot, excluding, ironically, this line\"]\n",
    "\n",
    "writer = wds.TarWriter(\"../data/test.tar\")\n",
    "\n",
    "Path(\"test\").mkdir(exist_ok=True)\n",
    "for i, text in enumerate(texts):\n",
    "    loss = opt_small.run_with_hooks(text, return_type=\"loss\", fwd_hooks=[(f'blocks.{mid_layer}.hook_resid_post', cache_it)])\n",
    "    writer.write({\n",
    "        \"__key__\": f\"{i:06d}\",\n",
    "        \"text.txt\": text,\n",
    "        \"values.npy\": cached_value.cpu().numpy()\n",
    "    })\n",
    "\n",
    "writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "70d4def6dfc8da35",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = wds.WebDataset(\"../data/test.tar\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b70dce6e16016f94",
   "metadata": {},
   "outputs": [],
   "source": [
    "for item in iter(dataset):\n",
    "    print(item[\"text.txt\"])\n",
    "    # Read from an npy file\n",
    "    values = np.load(BytesIO(item[\"values.npy\"]))\n",
    "    print(values.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5b4286bdaf79404",
   "metadata": {},
   "source": [
    "Ok now we want to prepare a large set of sentences from wikipedia."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6a92f6c205477e8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "wiki = datasets.load_dataset(\"wikipedia\", \"20220301.en\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "af70b4163cad995b",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(wiki[\"train\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "1a0ee88f16d6819a",
   "metadata": {},
   "outputs": [],
   "source": [
    "wiki[\"train\"][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "abeb017ae728e079",
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.download('punkt')\n",
    "nltk.download('punkt_tab')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "27ea1fe8234c5585",
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.sent_tokenize(wiki[\"train\"][0][\"text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "bafdf2c895f1a854",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_frac = 0.8\n",
    "test_frac = 1.0 - train_frac\n",
    "\n",
    "train_mask = np.random.rand(len(wiki[\"train\"])) < train_frac\n",
    "test_mask = ~train_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "61962dbcffbfb97c",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(train_mask[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "4922d31cd557abd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "sum(train_mask), sum(test_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "d78e9f9011ead5ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_inds = np.where(train_mask)[0]\n",
    "test_inds = np.where(test_mask)[0]\n",
    "\n",
    "train_inds = np.random.permutation(train_inds)\n",
    "test_inds = np.random.permutation(test_inds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "73ce3e2b854700b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.savez(\"../data/wiki_split.npz\", train_inds=train_inds, test_inds=test_inds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "f5f79aa3610135dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_inds[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "fa23fa798483fe92",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.notebook import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "8306efc6d1b88bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check a few random sentences on whether the tokenization is the same\n",
    "num_samples = 10000\n",
    "num_diff = 0\n",
    "for i in tqdm(train_inds[:num_samples]):\n",
    "    text = wiki[\"train\"][int(i)][\"text\"]\n",
    "    sent = nltk.sent_tokenize(text)\n",
    "    random_sent = np.random.choice(sent)\n",
    "    # print(random_sent)\n",
    "    opt_tokens = opt_small.to_str_tokens(random_sent)\n",
    "    gpt2_tokens = gpt2_small.to_str_tokens(random_sent)\n",
    "    opt_tokens = opt_tokens[1:]\n",
    "    gpt2_tokens = gpt2_tokens[1:]\n",
    "    if opt_tokens != gpt2_tokens:\n",
    "        num_diff += 1\n",
    "print(f\"Number of different tokenizations: {num_diff}/{num_samples}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "3f8af352fdd1521b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_middle_hook(model):\n",
    "    nlayers = model.W_K.shape[0]\n",
    "    mid_layer = nlayers // 2\n",
    "    return f'blocks.{mid_layer}.hook_resid_post'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "a5c2384e663dd9e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(get_middle_hook(opt_small))\n",
    "print(get_middle_hook(gpt2_small))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "edb3090f291c0fbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "6265bcd5022556ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_sentences = 5\n",
    "batch_size = 32\n",
    "max_tokens = 1024\n",
    "# num_samples = 100_000\n",
    "# num_samples = 10_000\n",
    "start_sample = 10_000\n",
    "end_sample = 30_000\n",
    "writer = wds.TarWriter(\"../data/wiki_sentences_30_60k.tar\")\n",
    "\n",
    "with open(f'../outputs/log_{datetime.now():%Y%m%d_%H%M%S}.txt', 'w') as log_file:\n",
    "    num_sampled = 0\n",
    "    num_mismatched = 0\n",
    "    for ii, i in tqdm(enumerate(train_inds[start_sample:end_sample]), total=start_sample - end_sample):\n",
    "        text = wiki[\"train\"][int(i)][\"text\"]\n",
    "        sent = nltk.sent_tokenize(text)\n",
    "\n",
    "        print(f\"{ii} before check len\", file=log_file, flush=True)\n",
    "        if len(sent) <= num_sentences:\n",
    "            continue\n",
    "        print(f\"{ii} after check len\", file=log_file, flush=True)\n",
    "        random_chunk = np.random.randint(0, len(sent) - num_sentences)\n",
    "        chunk = sent[random_chunk:random_chunk + num_sentences]\n",
    "        chunk = \" \".join(chunk)\n",
    "        opt_tokens = opt_small.to_str_tokens(chunk)\n",
    "        gpt2_tokens = gpt2_small.to_str_tokens(chunk)\n",
    "        print(f\"{ii} after composing the sent\", file=log_file, flush=True)\n",
    "        \n",
    "        if len(opt_tokens) > max_tokens or len(gpt2_tokens) > max_tokens:\n",
    "            continue\n",
    "            \n",
    "        print(f\"{ii} token check\", file=log_file, flush=True)\n",
    "            \n",
    "        if opt_tokens[1:] != gpt2_tokens[1:]:\n",
    "            num_mismatched += 1\n",
    "\n",
    "        num_sampled += 1\n",
    "        \n",
    "        gpt2_small.run_with_hooks(chunk, return_type=\"loss\", fwd_hooks=[(get_middle_hook(gpt2_small), cache_it)])\n",
    "        resid_gpt2 = cached_value.detach().cpu().numpy()\n",
    "        print(f\"{ii} gpt2\", file=log_file, flush=True)\n",
    "        opt_small.run_with_hooks(chunk, return_type=\"loss\", fwd_hooks=[(get_middle_hook(opt_small), cache_it)])\n",
    "        resid_opt = cached_value.detach().cpu().numpy()\n",
    "        print(f\"{ii} opt\", file=log_file, flush=True)\n",
    "        \n",
    "        # print(resid_gpt2.shape, resid_opt.shape)\n",
    "        \n",
    "        writer.write({\n",
    "            \"__key__\": f\"{num_sampled:08d}\",\n",
    "            \"text.txt\": chunk,\n",
    "            \"resid_gpt2.npy\": resid_gpt2,\n",
    "            \"resid_opt.npy\": resid_opt\n",
    "        })\n",
    "\n",
    "        print(f\"{ii} written\", file=log_file, flush=True)\n",
    "        \n",
    "    writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "d45b57092d788021",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-31T01:14:46.208695685Z",
     "start_time": "2024-08-30T18:16:10.789209Z"
    }
   },
   "outputs": [],
   "source": [
    "# import gc\n",
    "# gc.collect()\n",
    "# t.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ae7bf1aef5d86e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"{num_mismatched=} {num_sampled=}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
